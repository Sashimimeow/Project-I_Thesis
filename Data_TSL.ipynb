{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuDOp4jX7agWpyRi1WZ0qQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sashimimeow/Project-I_Thesis/blob/main/Data_TSL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation"
      ],
      "metadata": {
        "id": "Rb2t8gRm1zuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas"
      ],
      "metadata": {
        "id": "3boI-bOY14yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. อ่านข้อมูลจากไฟล์ Excel\n",
        "file_path = 'your_file_path.xlsx'  # ใส่ path ของไฟล์ Excel ของคุณ\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# 2. ตรวจสอบข้อมูลที่นำเข้า\n",
        "print(df.head())  # แสดงข้อมูล 5 แถวแรก\n"
      ],
      "metadata": {
        "id": "mfSHEBbA15Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# เข้าถึงคอลัมน์ Sentence_TH\n",
        "sentences_th = df['Sentence_TH']\n",
        "\n",
        "# แสดงผลข้อมูลในคอลัมน์\n",
        "for sentence in sentences_th:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "id": "pyR46wqI15VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp import word_tokenize\n",
        "\n",
        "# ทำ Tokenization ข้อความในคอลัมน์ Sentence_TH\n",
        "for sentence in sentences_th:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    print(tokens)\n"
      ],
      "metadata": {
        "id": "np_kO1Es42vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Keyword Extraction"
      ],
      "metadata": {
        "id": "ZapMMqvD16mV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyThaiNLP Get Started"
      ],
      "metadata": {
        "id": "3rsAGdgi0ucd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPuCV6S10ok5",
        "outputId": "2802492f-4ea5-4776-f428-b1f4f1714fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pythainlp\n",
            "  Downloading pythainlp-5.0.4-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2024.8.30)\n",
            "Downloading pythainlp-5.0.4-py3-none-any.whl (17.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pythainlp\n",
            "Successfully installed pythainlp-5.0.4\n",
            "Collecting epitran\n",
            "  Downloading epitran-1.25.1-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from epitran) (71.0.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from epitran) (2024.5.15)\n",
            "Collecting panphon>=0.20 (from epitran)\n",
            "  Downloading panphon-0.21.2-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: marisa-trie in /usr/local/lib/python3.10/dist-packages (from epitran) (1.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from epitran) (2.32.3)\n",
            "Collecting jamo (from epitran)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting g2pk (from epitran)\n",
            "  Downloading g2pK-0.9.4-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting unicodecsv (from panphon>=0.20->epitran)\n",
            "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.20.2 in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (1.26.4)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (0.8.1)\n",
            "Collecting munkres (from panphon>=0.20->epitran)\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl.metadata (980 bytes)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from g2pk->epitran) (3.8.1)\n",
            "Collecting konlpy (from g2pk->epitran)\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting python-mecab-ko (from g2pk->epitran)\n",
            "  Downloading python_mecab_ko-1.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (2024.8.30)\n",
            "Collecting JPype1>=0.7.0 (from konlpy->g2pk->epitran)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy->g2pk->epitran) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->g2pk->epitran) (4.66.5)\n",
            "Collecting python-mecab-ko-dic (from python-mecab-ko->g2pk->epitran)\n",
            "  Downloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy->g2pk->epitran) (24.1)\n",
            "Downloading epitran-1.25.1-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.1/184.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading panphon-0.21.2-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading g2pK-0.9.4-py3-none-any.whl (27 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Downloading python_mecab_ko-1.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (577 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.1/577.1 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl (34.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: unicodecsv\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10745 sha256=a0e9d12c72f1e6b8698e74e897294579e2f747474120a80760a0ff69d55ede87\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/ea/66/8e45247b09052a933eb1a680b7c64802298faba58aac9b346b\n",
            "Successfully built unicodecsv\n",
            "Installing collected packages: unicodecsv, python-mecab-ko-dic, munkres, jamo, python-mecab-ko, panphon, JPype1, konlpy, g2pk, epitran\n",
            "Successfully installed JPype1-1.5.0 epitran-1.25.1 g2pk-0.9.4 jamo-0.4.1 konlpy-0.6.0 munkres-1.1.4 panphon-0.21.2 python-mecab-ko-1.3.7 python-mecab-ko-dic-2.1.1.post2 unicodecsv-0.14.1\n"
          ]
        }
      ],
      "source": [
        "# # pip install required modules\n",
        "# # uncomment if running from colab\n",
        "# # see list of modules in `requirements` and `extras`\n",
        "# # in https://github.com/PyThaiNLP/pythainlp/blob/dev/setup.py\n",
        "\n",
        "!pip install pythainlp\n",
        "!pip install epitran"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "K5Yf9Yxh1wx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import PyThaiNLP"
      ],
      "metadata": {
        "id": "Hoi6ex-W02Ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pythainlp\n",
        "\n",
        "pythainlp.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KzdNvZV20uOG",
        "outputId": "262da6d3-4100-4c8b-c779-8cb92373aa1e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'5.0.4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence\n",
        "\n",
        "Default sentence tokenizer is \"crfcut\". Tokenization engine can be chosen ussing `engine=` parameter."
      ],
      "metadata": {
        "id": "9SbyWrXB1ClG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp import sent_tokenize\n",
        "\n",
        "text = (\"พระราชบัญญัติธรรมนูญการปกครองแผ่นดินสยามชั่วคราว พุทธศักราช ๒๔๗๕ \"\n",
        "        \"เป็นรัฐธรรมนูญฉบับชั่วคราว ซึ่งถือว่าเป็นรัฐธรรมนูญฉบับแรกแห่งราชอาณาจักรสยาม \"\n",
        "        \"ประกาศใช้เมื่อวันที่ 27 มิถุนายน พ.ศ. 2475 \"\n",
        "        \"โดยเป็นผลพวงหลังการปฏิวัติเมื่อวันที่ 24 มิถุนายน พ.ศ. 2475 โดยคณะราษฎร\")\n",
        "\n",
        "print(\"default (crfcut):\")\n",
        "print(sent_tokenize(text))\n",
        "print(\"\\nwhitespace+newline:\")\n",
        "print(sent_tokenize(text, engine=\"whitespace+newline\"))"
      ],
      "metadata": {
        "id": "6qS0O7N81ARn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word\n",
        "Default word tokenizer (\"newmm\") use maximum matching algorithm."
      ],
      "metadata": {
        "id": "85N-y54J1IbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp import word_tokenize\n",
        "\n",
        "text = \"ก็จะรู้ความชั่วร้ายที่ทำไว้     และคงจะไม่ยอมให้ทำนาบนหลังคน \"\n",
        "\n",
        "print(\"default (newmm):\")\n",
        "print(word_tokenize(text))\n",
        "print(\"\\nnewmm and keep_whitespace=False:\")\n",
        "print(word_tokenize(text, keep_whitespace=False))"
      ],
      "metadata": {
        "id": "exBtpFnr1Hov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other algorithm can be chosen. We can also create a tokenizer with a custom dictionary."
      ],
      "metadata": {
        "id": "F-zHilJA1Rka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Tokenization\n",
        " แบ่งข้อความเป็นหน่วยย่อย เช่น คำหรือวลี"
      ],
      "metadata": {
        "id": "Miu4yoah3uMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pythainlp import word_tokenize, Tokenizer\n",
        "\n",
        "text = \"กฎหมายแรงงานฉบับปรับปรุงใหม่ประกาศใช้แล้ว\"\n",
        "\n",
        "print(\"newmm  :\", word_tokenize(text))  # default engine is \"newmm\"\n",
        "print(\"longest:\", word_tokenize(text, engine=\"longest\"))\n",
        "\n",
        "words = [\"แรงงาน\"]\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"newmm (custom dictionary):\", custom_tokenizer.word_tokenize(text))"
      ],
      "metadata": {
        "id": "8MC_2xdx1GH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Stopwords Removal\n",
        "ลบคำที่ไม่มีความสำคัญออก เช่น คำว่า \"ที่\", \"เป็น\", \"ของ\" ฯลฯ"
      ],
      "metadata": {
        "id": "8bqOJVoy30qF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Stemming/Lemmatization\n",
        "นำคำกลับสู่รูปแบบคำดั้งเดิม เช่น กำจัดการผันคำ"
      ],
      "metadata": {
        "id": "TrYNY6813993"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nDkKfR4Y1TOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OkTbSptG3szz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Learning"
      ],
      "metadata": {
        "id": "X4sGPZWf2WGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow\n"
      ],
      "metadata": {
        "id": "qkRWMLsJ2Uk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# สร้างโมเดลแปลงคำเป็นภาษามือ\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=64, input_length=10))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(5000, activation='softmax'))\n",
        "\n",
        "# สรุปโมเดล\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "QVvwub8-2iLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "text = \"ฉันต้องการพัฒนาโมเดลการแปลภาษามือไทย\"\n",
        "nlp = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
        "keywords = nlp(text)\n",
        "print(keywords)\n"
      ],
      "metadata": {
        "id": "wjqoThuO3jFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Video Mapping"
      ],
      "metadata": {
        "id": "GEgFpA2T2bFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ตัวอย่างคลังคำและวิดีโอที่เชื่อมโยงกัน\n",
        "word_to_video = {\n",
        "    \"ไป\": \"video/go.mp4\",\n",
        "    \"เที่ยว\": \"video/travel.mp4\",\n",
        "    \"ภูเก็ต\": \"video/phuket.mp4\"\n",
        "}\n",
        "\n",
        "# ฟังก์ชันการเชื่อมโยงคำกับวิดีโอ\n",
        "def get_video_for_words(words):\n",
        "    videos = []\n",
        "    for word in words:\n",
        "        if word in word_to_video:\n",
        "            videos.append(word_to_video[word])\n",
        "    return videos\n",
        "\n",
        "# คำที่ได้จากการแปล\n",
        "translated_words = [\"ไป\", \"เที่ยว\", \"ภูเก็ต\"]\n",
        "video_files = get_video_for_words(translated_words)\n",
        "print(\"วิดีโอที่ใช้:\", video_files)\n"
      ],
      "metadata": {
        "id": "rYCxohvo2cVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tHHUHiKO2cga"
      }
    }
  ]
}